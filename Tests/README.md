# Tests - Suite Completa de Pruebas

## ğŸ“‹ DescripciÃ³n

Este directorio contiene una suite completa de tests organizados para el sistema de Chatbot de Emergencias. Los tests estÃ¡n diseÃ±ados para mejorar la cobertura del cÃ³digo desde el baseline de 58% hasta 75%+ con enfoque en calidad de prototipo.

---

## ğŸ“ Estructura de Tests

```
Tests/
â”œâ”€â”€ test_chatbot_service.py      # 30+ tests - Servicio principal (ChatbotService)
â”œâ”€â”€ test_rag_system.py            # 35+ tests - Sistema RAG completo
â”œâ”€â”€ test_api_integration.py       # 40+ tests - IntegraciÃ³n E2E de APIs
â”œâ”€â”€ test_error_handling.py        # 50+ tests - Manejo de errores y edge cases
â”œâ”€â”€ test_models_extended.py       # 45+ tests - Modelos extendidos
â””â”€â”€ README.md                     # Este archivo
```

**Total: ~200 tests adicionales**

---

## ğŸ¯ Objetivos de Cobertura

| Componente | Cobertura Inicial | Meta | Tests Enfocados |
|------------|-------------------|------|-----------------|
| **chatbot_service.py** | 43% | 75%+ | test_chatbot_service.py |
| **Sistema RAG** | 32-42% | 70%+ | test_rag_system.py |
| **Views/API** | 58% | 75%+ | test_api_integration.py |
| **Models** | 90% | 95%+ | test_models_extended.py |
| **Error Handling** | N/A | 80%+ | test_error_handling.py |

---

## ğŸ§ª DescripciÃ³n de Archivos

### 1. test_chatbot_service.py
**Objetivo**: Cobertura del servicio principal de chatbot (43% â†’ 75%+)

**Incluye:**
- âœ… Tests de inicializaciÃ³n del servicio
- âœ… Tests de `start_conversation()`
- âœ… Tests de `process_message()`
- âœ… Tests de `_extract_data_with_llm()`
- âœ… Tests de transiciones de estado
- âœ… Tests de cÃ¡lculo de prioridad
- âœ… Tests de creaciÃ³n de emergencias
- âœ… Tests de historial de conversaciÃ³n
- âœ… Tests de manejo de errores
- âœ… Tests de integraciÃ³n con RAG

**Tests totales: ~30**

**Ejemplo de test:**
```python
@patch('ModuloEmergencia.services.chatbot_service.ChatbotService._generate_llm_response')
def test_start_conversation_creates_new_session(self, mock_llm):
    mock_llm.return_value = "Â¡Hola! Â¿En quÃ© puedo ayudarte?"
    
    service = ChatbotService()
    session_id, estado, respuesta = service.start_conversation()
    
    self.assertIsNotNone(session_id)
    self.assertEqual(estado, 'iniciada')
    self.assertIn("Hola", respuesta)
```

---

### 2. test_rag_system.py
**Objetivo**: Cobertura del sistema RAG (32-42% â†’ 70%+)

**Incluye:**
- âœ… Tests de `vector_store.py` (VectorStore, ChromaDB)
- âœ… Tests de `embeddings.py` (DocumentProcessor)
- âœ… Tests de `retriever.py` (RAGRetriever)
- âœ… Tests de `ingest_documents.py` (ingesta de documentos)
- âœ… Tests de calidad de embeddings
- âœ… Tests de bÃºsqueda semÃ¡ntica
- âœ… Tests de filtros por categorÃ­a
- âœ… Tests de manejo de resultados vacÃ­os
- âœ… Tests de divisiÃ³n de documentos (chunking)

**Tests totales: ~35**

**Ejemplo de test:**
```python
def test_similar_texts_have_similar_embeddings(self):
    processor = DocumentProcessor()
    
    text1 = "Tengo una fuga de agua en mi casa"
    text2 = "Hay una fuga de agua en mi domicilio"
    text3 = "El clima estÃ¡ soleado hoy"
    
    embeddings = processor.generate_embeddings([text1, text2, text3])
    
    # Textos similares deben tener mayor similitud
    similarity_12 = cosine_similarity(embeddings[0], embeddings[1])
    similarity_13 = cosine_similarity(embeddings[0], embeddings[2])
    
    self.assertGreater(similarity_12, similarity_13)
```

---

### 3. test_api_integration.py
**Objetivo**: Tests de integraciÃ³n end-to-end (E2E)

**Incluye:**
- âœ… Tests de flujo completo de chat
- âœ… Tests de endpoints de emergencias
- âœ… Tests de estadÃ­sticas
- âœ… Tests de filtros y bÃºsquedas
- âœ… Tests de historial de conversaciÃ³n
- âœ… Tests de manejo de errores en API
- âœ… Tests de validaciÃ³n de datos
- âœ… Tests de concurrencia
- âœ… Tests de mÃºltiples sesiones

**Tests totales: ~40**

**Ejemplo de test:**
```python
@patch('ModuloEmergencia.services.chatbot_service.ChatbotService._generate_llm_response')
def test_complete_emergency_report_flow(self, mock_llm):
    mock_llm.return_value = "Entendido"
    
    # 1. Iniciar conversaciÃ³n
    response = self.client.post('/api/emergencias/chat/init/')
    session_id = response.data['session_id']
    
    # 2. Recolectar datos
    conversation_steps = [
        "Estoy en Anibana",
        "Hay una fuga de agua",
        "Calle Principal 123",
        "Juan PÃ©rez",
        "+56912345678"
    ]
    
    for mensaje in conversation_steps:
        response = self.client.post('/api/emergencias/chat/message/', {
            'session_id': session_id,
            'mensaje': mensaje
        })
        self.assertEqual(response.status_code, status.HTTP_200_OK)
    
    # 3. Verificar que los datos se acumularon
    conversation = ChatConversation.objects.get(session_id=session_id)
    self.assertIn('sector', conversation.datos_recolectados)
```

---

### 4. test_error_handling.py
**Objetivo**: Robustez y manejo de casos edge

**Incluye:**
- âœ… Tests de validaciÃ³n de inputs (vacÃ­os, muy largos, caracteres especiales)
- âœ… Tests de protecciÃ³n contra SQL injection
- âœ… Tests de protecciÃ³n contra XSS
- âœ… Tests de errores de la API de Gemini (timeout, rate limit, API key invÃ¡lida)
- âœ… Tests de errores de JSON invÃ¡lido
- âœ… Tests de errores de estado de conversaciÃ³n
- âœ… Tests de datos incompletos o invÃ¡lidos
- âœ… Tests de errores del sistema RAG
- âœ… Tests de casos edge (prioridades mixtas, formatos de telÃ©fono)
- âœ… Tests de lÃ­mites de recursos

**Tests totales: ~50**

**Ejemplo de test:**
```python
@patch('google.generativeai.GenerativeModel.generate_content')
def test_invalid_json_from_llm(self, mock_gemini):
    mock_response = Mock()
    mock_response.text = "Este no es JSON vÃ¡lido {{{["
    mock_gemini.return_value = mock_response
    
    result = self.service._extract_data_with_llm("Mensaje", session_id)
    
    # Debe retornar dict vacÃ­o en lugar de lanzar excepciÃ³n
    self.assertIsInstance(result, dict)
    self.assertEqual(len(result), 0)
```

---

### 5. test_models_extended.py
**Objetivo**: Cobertura completa de modelos (90% â†’ 95%+)

**Incluye:**
- âœ… Tests extendidos de `Emergencia` (todos los sectores, tipos, prioridades)
- âœ… Tests extendidos de `ChatConversation` (todos los estados)
- âœ… Tests extendidos de `ChatMessage` (roles, timestamps)
- âœ… Tests de representaciones string
- âœ… Tests de campos auto-generados (timestamps, UUIDs)
- âœ… Tests de relaciones entre modelos
- âœ… Tests de cascade delete
- âœ… Tests de unicidad de campos
- âœ… Tests de QuerySets y filtros

**Tests totales: ~45**

**Ejemplo de test:**
```python
def test_emergencia_all_sectors(self):
    sectores = [
        'anibana', 'pedro_aguirre_cerda', 'villa_san_jose',
        'el_molino', 'el_laurel', 'huara', 'punta_patache'
    ]
    
    for sector in sectores:
        emergencia = Emergencia.objects.create(
            sector=sector,
            tipo_emergencia='fuga_agua',
            descripcion='Test',
            # ... otros campos
        )
        self.assertEqual(emergencia.sector, sector)
```

---

## ğŸš€ CÃ³mo Ejecutar los Tests

### Ejecutar todos los tests

```bash
# Desde la raÃ­z del proyecto Backend
python manage.py test Tests

# O con pytest
pytest Tests/
```

### Ejecutar un archivo especÃ­fico

```bash
# Test del servicio de chatbot
python manage.py test Tests.test_chatbot_service

# Test del sistema RAG
python manage.py test Tests.test_rag_system

# Test de integraciÃ³n API
python manage.py test Tests.test_api_integration

# Test de manejo de errores
python manage.py test Tests.test_error_handling

# Test de modelos extendidos
python manage.py test Tests.test_models_extended
```

### Ejecutar con cobertura

```bash
# Generar reporte de cobertura
coverage run --source='ModuloEmergencia' manage.py test Tests
coverage report
coverage html

# Ver reporte HTML
# Abrir htmlcov/index.html en el navegador
```

### Ejecutar tests especÃ­ficos

```bash
# Ejecutar una clase de tests
python manage.py test Tests.test_chatbot_service.StartConversationTests

# Ejecutar un test individual
python manage.py test Tests.test_chatbot_service.StartConversationTests.test_start_conversation_creates_new_session
```

---

## ğŸ“Š Cobertura Esperada

DespuÃ©s de ejecutar todos los tests, la cobertura esperada es:

| Componente | LÃ­neas | Cobertura |
|------------|--------|-----------|
| chatbot_service.py | 534 | 75%+ |
| vector_store.py | 150 | 70%+ |
| embeddings.py | 200 | 70%+ |
| retriever.py | 180 | 70%+ |
| views.py | 300 | 75%+ |
| models.py | 120 | 95%+ |
| **TOTAL** | 1484+ | **75%+** |

---

## ğŸ”§ ConfiguraciÃ³n de Tests

### Settings para Tests

Los tests usan la configuraciÃ³n de `settings.py` con algunas sobrescrituras automÃ¡ticas de Django:

- Base de datos en memoria (SQLite)
- `DEBUG = True`
- Mocks para servicios externos (Gemini, ChromaDB cuando es necesario)

### Fixtures

Si necesitas datos de prueba consistentes, crea fixtures:

```bash
# Exportar datos actuales
python manage.py dumpdata ModuloEmergencia > Tests/fixtures/test_data.json

# Cargar fixtures en tests
class MyTestCase(TestCase):
    fixtures = ['test_data.json']
```

---

## ğŸ¯ Estrategia de Testing

### 1. **Unit Tests** (Unitarios)
- Prueban funciones/mÃ©todos individuales
- Usan mocks para dependencias externas
- RÃ¡pidos de ejecutar
- Ejemplos: `test_chatbot_service.py`, `test_models_extended.py`

### 2. **Integration Tests** (IntegraciÃ³n)
- Prueban interacciÃ³n entre componentes
- Usan base de datos real (in-memory)
- Ejemplos: `test_api_integration.py`

### 3. **Edge Case Tests** (Casos Edge)
- Prueban lÃ­mites y casos inusuales
- ValidaciÃ³n de robustez
- Ejemplos: `test_error_handling.py`

---

## ğŸ› Debugging Tests

### Ver output detallado

```bash
# Verbose mode
python manage.py test Tests --verbosity=2

# Con pytest, ver prints
pytest Tests/ -s

# Ver solo tests que fallan
pytest Tests/ --tb=short
```

### Ejecutar solo tests que fallaron

```bash
# Con pytest
pytest Tests/ --lf  # last-failed
pytest Tests/ --ff  # failed-first
```

---

## ğŸ“ GuÃ­as de Testing

### Escribir Nuevos Tests

1. **Elegir el archivo correcto**:
   - Servicio de chatbot â†’ `test_chatbot_service.py`
   - Sistema RAG â†’ `test_rag_system.py`
   - API REST â†’ `test_api_integration.py`
   - Errores/Edge cases â†’ `test_error_handling.py`
   - Modelos â†’ `test_models_extended.py`

2. **Estructura bÃ¡sica**:
```python
class MyFeatureTests(TestCase):
    """Tests para mi feature"""
    
    def setUp(self):
        """Preparar datos de prueba"""
        self.data = ...
    
    def test_feature_works(self):
        """Test que la feature funciona correctamente"""
        result = my_function(self.data)
        self.assertEqual(result, expected_value)
    
    def tearDown(self):
        """Limpiar despuÃ©s del test"""
        pass
```

3. **Usar mocks para servicios externos**:
```python
@patch('path.to.external.service')
def test_with_mock(self, mock_service):
    mock_service.return_value = "mocked response"
    result = my_function()
    self.assertEqual(result, "expected")
```

---

## ğŸ” Tests Pendientes

Ãreas que podrÃ­an necesitar mÃ¡s tests en el futuro:

- [ ] Tests de performance (tiempo de respuesta)
- [ ] Tests de carga (mÃºltiples usuarios simultÃ¡neos)
- [ ] Tests de seguridad mÃ¡s exhaustivos
- [ ] Tests de UI (si se agrega interfaz web en backend)
- [ ] Tests de migraciÃ³n de datos
- [ ] Tests de backup/restore

---

## ğŸ“š Referencias

- **Django Testing**: https://docs.djangoproject.com/en/5.0/topics/testing/
- **DRF Testing**: https://www.django-rest-framework.org/api-guide/testing/
- **unittest.mock**: https://docs.python.org/3/library/unittest.mock.html
- **Coverage.py**: https://coverage.readthedocs.io/
- **pytest-django**: https://pytest-django.readthedocs.io/

---

## ğŸ¤ Contribuir con Tests

Al agregar nuevas funcionalidades:

1. âœ… Escribir tests **antes** de implementar (TDD)
2. âœ… Asegurar cobertura > 70% del cÃ³digo nuevo
3. âœ… Incluir tests de casos edge y errores
4. âœ… Documentar tests complejos con comentarios
5. âœ… Verificar que todos los tests pasen antes de commit

---

## ğŸ“Š Reporte de Cobertura

Para generar un reporte completo:

```bash
# 1. Ejecutar tests con cobertura
coverage run --source='ModuloEmergencia' manage.py test Tests

# 2. Ver reporte en consola
coverage report

# 3. Generar reporte HTML detallado
coverage html

# 4. Ver en navegador
# Abrir htmlcov/index.html
```

El reporte mostrarÃ¡:
- LÃ­neas totales vs cubiertas
- Porcentaje de cobertura por archivo
- LÃ­neas especÃ­ficas no cubiertas (resaltadas en rojo)

---

**Ãšltima actualizaciÃ³n**: 5 de Diciembre, 2025  
**VersiÃ³n**: 1.0.0  
**Tests totales**: ~200 tests adicionales  
**Objetivo de cobertura**: 75%+ (desde 58% baseline)
